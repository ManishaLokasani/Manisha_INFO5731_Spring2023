{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManishaLokasani/Manisha_INFO5731_Spring2023/blob/main/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using a hashtag (you can use any hashtag) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re \n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "pages = int(input(\"Enter num of pages you want to process: \"))\n",
        "amazon_reviews=[]\n",
        "for i in range(pages):\n",
        "  url=f\"https://www.amazon.com/product-reviews/B07YFG5TBX/ref=cm_cr_getr_d_paging_btm_prev_{i}?ie=UTF8&filterByStar=five_star&reviewerType=all_reviews&pageNumber={i}#reviews-filter-bar\"\n",
        "  headers = {\n",
        "      'authority': 'www.amazon.com',\n",
        "      'user-agent' :'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'\n",
        "      }\n",
        "  amazon_data = reiquests.get(url, headers=headers)\n",
        "  soup = BeautifulSoup(amazon_data.content, \"html.parser\")\n",
        "  # print(soup.prettify())\n",
        "  products_data=soup.findAll(\"span\", attrs={\"class\": \"a-size-base review-text review-text-content\"})\n",
        "  products_details=soup.findAll(\"span\", attrs={\"class\": \"a-size-base \"})\n",
        "  for data in products_data:\n",
        "    x=data.text\n",
        "    amazon_reviews.append(x)\n",
        "    print(x)\n",
        "  for data in products_details:\n",
        "    y=data.text\n",
        "    #amazon_reviews.append(y)\n",
        "    print(y)\n",
        "  print(f\"Processed page: {(i+1)}\")\n",
        "\n",
        "#print(amazon_reviews)\n",
        " \n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXqoSnbSkTBy",
        "outputId": "c731cf73-c565-494f-e637-cc20ce21ec40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter num of pages you want to process: 1\n",
            "\n",
            "My little one loved this backpack.  She wore it on Xmas and packed to go to her Nonna's house.\n",
            "\n",
            "\n",
            "Su material es excelente, a mi hija le encanta, lo trae jugando y aparte lo ocupo como paÃ±alera si voy de salida y regreso pronto. Le caben sus toallitas, 2 paÃ±ales y hasta un juguete si ella quiere. Lo recomiendo\n",
            "\n",
            "\n",
            "Gifted to my friends daughter, itâ€™s too cute ðŸ¥°\n",
            "\n",
            "\n",
            "My daughter loves itâ€¦\n",
            "\n",
            "\n",
            "Items exactly matches web image and color too is the same. Looks cute and seems sturdy. Hope it will withstand time with the kid. The only minor thing is the bag is slightly smaller than i expected. But not a huge issue. It shouldn't be large as it is for a kid. But could have been a couple inches taller. Very good buy\n",
            "\n",
            "\n",
            "It is well made. Fabric is beautiful.My granddaughter loves it . She hugged and kissed it as soon as she opened itShe takes it with her most of the time,  wears it around the house.\n",
            "\n",
            "\n",
            "Love this yummy backpackwashes in the machine and keeps stronghighly recommended\n",
            "\n",
            "\n",
            "My nephew & family sent this product as gift for my daughter. My 2 year old baby loves the bag from the moment I opened it and not allowing us to remove it from her shoulder back. The product is so soft and adorable that everybody in my home liked it very much especially my baby.\n",
            "\n",
            "\n",
            "Our granddaughter is 3 and loved it\n",
            "\n",
            "\n",
            "I purchased this for my granddaughter and she absolutely loves it! More then I expected, I love how soft it is and it super bright- which makes it easy to find in the morning for daycare.Fun fact: The little girl resembles my granddaughter so much!\n",
            "\n",
            "Processed page: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'User Reviews': amazon_reviews})\n",
        "\n",
        "#save the dataframe on a csv file\n",
        "df.to_csv('userReviews.csv')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o_ZYsgbDqEYE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "84be6857-2d2e-4c7d-df77-51673a476beb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "rev = []\n",
        "for i in amazon_reviews:\n",
        "  s = str(i)\n",
        "  x = s.replace(s[:37],\"\").replace(\"<br/><br/>\",\"\").replace(\"</div>\", \"\").replace(\"\\n\", \" \")\n",
        "  rev.append(x.lower())\n",
        "\n",
        "\n",
        "\n",
        "def Punctuation(rev):\n",
        "    punctuation = string.punctuation\n",
        "    lis = []\n",
        "    for sentence in rev:\n",
        "      se=[]\n",
        "      se.clear()\n",
        "      for j in sentence:\n",
        "        #print(j)\n",
        "        if j in punctuation:\n",
        "          se.append(\"\")        \n",
        "        else:\n",
        "          se.append(j)  \n",
        "      lis.append(\"\".join(se).replace(\"i \", \"\"))\n",
        "      \n",
        "       \n",
        "    return lis\n",
        "      \n",
        "def convert_sentence(rev):\n",
        "  reviews=[]\n",
        "  for i in rev:\n",
        "    reviews.append(i)\n",
        "  return reviews\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'User Reviews': Punctuation(rev) })\n",
        "\n",
        "#save the dataframe on a csv file\n",
        "df.to_csv('userReviews.csv')\n",
        "\n",
        "\n",
        "\n",
        "df[\"User Reviews\"][0][3]\n",
        "      \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_WtM1YtCbJl3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQKnPjPDHJHr"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}